{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš¦ Vision Model Evaluator for Traffic Sign Recognition\n",
        "\n",
        "This notebook provides a comprehensive toolkit for evaluating vision-language models on traffic sign recognition tasks.\n",
        "\n",
        "## Supported Models\n",
        "- **Google Gemini** (replaces deprecated Bard API)\n",
        "- **OpenAI GPT-4V/GPT-4o**\n",
        "- **LLaVA** (local deployment)\n",
        "\n",
        "## Features\n",
        "- âœ… Async processing for improved throughput\n",
        "- âœ… Automatic retry with exponential backoff\n",
        "- âœ… Progress tracking with tqdm\n",
        "- âœ… Comprehensive logging\n",
        "- âœ… Results export to CSV/JSON with metadata\n",
        "- âœ… Batch processing with rate limiting\n",
        "- âœ… Result visualization and analysis"
      ],
      "metadata": {
        "id": "header-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¦ Installation"
      ],
      "metadata": {
        "id": "install-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-cell"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q httpx tqdm pillow pandas matplotlib seaborn\n",
        "\n",
        "# For Google Drive integration in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Configuration"
      ],
      "metadata": {
        "id": "config-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field, asdict\n",
        "from pathlib import Path\n",
        "from typing import Optional, Any\n",
        "from enum import Enum\n",
        "import os\n",
        "\n",
        "class ModelType(str, Enum):\n",
        "    GEMINI = \"gemini\"\n",
        "    GPT4V = \"gpt4v\"\n",
        "    LLAVA = \"llava\"\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Main configuration for the evaluation.\"\"\"\n",
        "    # Model settings\n",
        "    model_type: ModelType = ModelType.GPT4V\n",
        "    api_key: str = \"\"  # Set via environment or directly\n",
        "    \n",
        "    # Paths\n",
        "    image_folder: str = \"/content/drive/MyDrive/traffic_signs\"\n",
        "    output_folder: str = \"/content/drive/MyDrive/results\"\n",
        "    \n",
        "    # Evaluation settings\n",
        "    prompt: str = (\n",
        "        \"Is the traffic sign displayed a real-world traffic sign that has the same \"\n",
        "        \"shape, color, pattern and text as a real world traffic sign? \"\n",
        "        \"Answer with 'yes' or 'no' and provide a brief explanation.\"\n",
        "    )\n",
        "    \n",
        "    # Rate limiting\n",
        "    delay_between_requests: float = 1.0  # seconds\n",
        "    max_retries: int = 3\n",
        "    \n",
        "# Create config instance\n",
        "config = Config()\n",
        "\n",
        "# Set API key from environment or enter directly\n",
        "# Option 1: From environment\n",
        "config.api_key = os.environ.get('OPENAI_API_KEY', '') or os.environ.get('GEMINI_API_KEY', '')\n",
        "\n",
        "# Option 2: Enter directly (uncomment and fill in)\n",
        "# config.api_key = \"your-api-key-here\"\n",
        "\n",
        "print(f\"Model: {config.model_type.value}\")\n",
        "print(f\"API Key configured: {'Yes' if config.api_key else 'No'}\")\n",
        "print(f\"Image folder: {config.image_folder}\")"
      ],
      "metadata": {
        "id": "config-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Core Utilities"
      ],
      "metadata": {
        "id": "utils-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import asyncio\n",
        "import httpx\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def encode_image(image_path: str) -> str:\n",
        "    \"\"\"Encode image to base64.\"\"\"\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode('utf-8')\n",
        "\n",
        "def get_mime_type(path: str) -> str:\n",
        "    \"\"\"Get MIME type from file extension.\"\"\"\n",
        "    ext = Path(path).suffix.lower()\n",
        "    return {\n",
        "        '.jpg': 'image/jpeg',\n",
        "        '.jpeg': 'image/jpeg',\n",
        "        '.png': 'image/png',\n",
        "        '.gif': 'image/gif',\n",
        "        '.webp': 'image/webp'\n",
        "    }.get(ext, 'image/jpeg')\n",
        "\n",
        "def discover_images(folder: str) -> list[str]:\n",
        "    \"\"\"Find all images in folder.\"\"\"\n",
        "    extensions = ('.jpg', '.jpeg', '.png', '.gif', '.webp')\n",
        "    folder_path = Path(folder)\n",
        "    images = []\n",
        "    for ext in extensions:\n",
        "        images.extend(folder_path.glob(f'*{ext}'))\n",
        "        images.extend(folder_path.glob(f'*{ext.upper()}'))\n",
        "    return sorted([str(p) for p in set(images)])\n",
        "\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    \"\"\"Single evaluation result.\"\"\"\n",
        "    image_name: str\n",
        "    prediction: str\n",
        "    raw_response: str\n",
        "    latency_ms: float\n",
        "    success: bool\n",
        "    error: Optional[str] = None\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "print(\"âœ… Utilities loaded\")"
      ],
      "metadata": {
        "id": "utils-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¤– Model Clients"
      ],
      "metadata": {
        "id": "models-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseClient(ABC):\n",
        "    \"\"\"Base class for model clients.\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, timeout: float = 60.0):\n",
        "        self.api_key = api_key\n",
        "        self.timeout = timeout\n",
        "        self.client: Optional[httpx.AsyncClient] = None\n",
        "    \n",
        "    async def __aenter__(self):\n",
        "        self.client = httpx.AsyncClient(timeout=self.timeout)\n",
        "        return self\n",
        "    \n",
        "    async def __aexit__(self, *args):\n",
        "        if self.client:\n",
        "            await self.client.aclose()\n",
        "    \n",
        "    @abstractmethod\n",
        "    async def analyze(self, image_path: str, prompt: str) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class GeminiClient(BaseClient):\n",
        "    \"\"\"Google Gemini API client.\"\"\"\n",
        "    \n",
        "    MODEL = \"gemini-1.5-flash\"\n",
        "    BASE_URL = \"https://generativelanguage.googleapis.com/v1beta\"\n",
        "    \n",
        "    async def analyze(self, image_path: str, prompt: str) -> str:\n",
        "        image_data = encode_image(image_path)\n",
        "        mime_type = get_mime_type(image_path)\n",
        "        \n",
        "        url = f\"{self.BASE_URL}/models/{self.MODEL}:generateContent?key={self.api_key}\"\n",
        "        \n",
        "        payload = {\n",
        "            \"contents\": [{\n",
        "                \"parts\": [\n",
        "                    {\"text\": prompt},\n",
        "                    {\"inline_data\": {\"mime_type\": mime_type, \"data\": image_data}}\n",
        "                ]\n",
        "            }],\n",
        "            \"generationConfig\": {\"temperature\": 0.1, \"maxOutputTokens\": 100}\n",
        "        }\n",
        "        \n",
        "        response = await self.client.post(url, json=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        \n",
        "        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "\n",
        "\n",
        "class GPT4VClient(BaseClient):\n",
        "    \"\"\"OpenAI GPT-4 Vision client.\"\"\"\n",
        "    \n",
        "    MODEL = \"gpt-4o\"  # or \"gpt-4-vision-preview\" for older API\n",
        "    BASE_URL = \"https://api.openai.com/v1\"\n",
        "    \n",
        "    async def analyze(self, image_path: str, prompt: str) -> str:\n",
        "        image_data = encode_image(image_path)\n",
        "        mime_type = get_mime_type(image_path)\n",
        "        \n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        \n",
        "        payload = {\n",
        "            \"model\": self.MODEL,\n",
        "            \"messages\": [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:{mime_type};base64,{image_data}\",\n",
        "                            \"detail\": \"high\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }],\n",
        "            \"max_tokens\": 100,\n",
        "            \"temperature\": 0.1\n",
        "        }\n",
        "        \n",
        "        response = await self.client.post(\n",
        "            f\"{self.BASE_URL}/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=payload\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        \n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def get_client(model_type: ModelType, api_key: str) -> BaseClient:\n",
        "    \"\"\"Factory function to get appropriate client.\"\"\"\n",
        "    if model_type == ModelType.GEMINI:\n",
        "        return GeminiClient(api_key)\n",
        "    elif model_type == ModelType.GPT4V:\n",
        "        return GPT4VClient(api_key)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_type}\")\n",
        "\n",
        "print(\"âœ… Model clients loaded\")"
      ],
      "metadata": {
        "id": "models-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸƒ Evaluation Runner"
      ],
      "metadata": {
        "id": "eval-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    \"\"\"Main evaluation runner.\"\"\"\n",
        "    \n",
        "    def __init__(self, client: BaseClient, config: Config):\n",
        "        self.client = client\n",
        "        self.config = config\n",
        "        self.results: list[EvalResult] = []\n",
        "    \n",
        "    def _extract_prediction(self, response: str) -> str:\n",
        "        \"\"\"Extract yes/no from response.\"\"\"\n",
        "        resp = response.lower().strip()\n",
        "        if resp.startswith('yes'):\n",
        "            return 'yes'\n",
        "        elif resp.startswith('no'):\n",
        "            return 'no'\n",
        "        # Check for yes/no anywhere\n",
        "        if resp.count('yes') > resp.count('no'):\n",
        "            return 'yes'\n",
        "        elif resp.count('no') > resp.count('yes'):\n",
        "            return 'no'\n",
        "        return 'unclear'\n",
        "    \n",
        "    async def evaluate_single(self, image_path: str) -> EvalResult:\n",
        "        \"\"\"Evaluate single image with retries.\"\"\"\n",
        "        image_name = Path(image_path).name\n",
        "        start = time.perf_counter()\n",
        "        \n",
        "        for attempt in range(1, self.config.max_retries + 1):\n",
        "            try:\n",
        "                response = await self.client.analyze(image_path, self.config.prompt)\n",
        "                latency = (time.perf_counter() - start) * 1000\n",
        "                \n",
        "                return EvalResult(\n",
        "                    image_name=image_name,\n",
        "                    prediction=self._extract_prediction(response),\n",
        "                    raw_response=response,\n",
        "                    latency_ms=latency,\n",
        "                    success=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Attempt {attempt} failed for {image_name}: {e}\")\n",
        "                if attempt < self.config.max_retries:\n",
        "                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
        "                else:\n",
        "                    latency = (time.perf_counter() - start) * 1000\n",
        "                    return EvalResult(\n",
        "                        image_name=image_name,\n",
        "                        prediction='error',\n",
        "                        raw_response='',\n",
        "                        latency_ms=latency,\n",
        "                        success=False,\n",
        "                        error=str(e)\n",
        "                    )\n",
        "        \n",
        "        raise RuntimeError(\"Should not reach here\")\n",
        "    \n",
        "    async def run(self, images: list[str]) -> list[EvalResult]:\n",
        "        \"\"\"Run evaluation on all images.\"\"\"\n",
        "        for image_path in tqdm(images, desc=\"Evaluating\"):\n",
        "            result = await self.evaluate_single(image_path)\n",
        "            self.results.append(result)\n",
        "            await asyncio.sleep(self.config.delay_between_requests)\n",
        "        \n",
        "        return self.results\n",
        "    \n",
        "    def save_results(self, output_folder: str):\n",
        "        \"\"\"Save results to CSV and JSON.\"\"\"\n",
        "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        \n",
        "        # CSV\n",
        "        csv_path = Path(output_folder) / f\"results_{timestamp}.csv\"\n",
        "        with open(csv_path, 'w', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['image_name', 'prediction', 'raw_response', 'latency_ms', 'success', 'error'])\n",
        "            writer.writeheader()\n",
        "            for r in self.results:\n",
        "                writer.writerow(asdict(r))\n",
        "        \n",
        "        # JSON\n",
        "        json_path = Path(output_folder) / f\"results_{timestamp}.json\"\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump({\n",
        "                'metadata': {\n",
        "                    'model': config.model_type.value,\n",
        "                    'total_images': len(self.results),\n",
        "                    'timestamp': timestamp\n",
        "                },\n",
        "                'results': [asdict(r) for r in self.results]\n",
        "            }, f, indent=2)\n",
        "        \n",
        "        print(f\"âœ… Results saved to:\\n   - {csv_path}\\n   - {json_path}\")\n",
        "        return csv_path, json_path\n",
        "    \n",
        "    def summary(self) -> dict:\n",
        "        \"\"\"Get summary statistics.\"\"\"\n",
        "        successful = [r for r in self.results if r.success]\n",
        "        predictions = [r.prediction for r in successful]\n",
        "        latencies = [r.latency_ms for r in successful]\n",
        "        \n",
        "        return {\n",
        "            'total': len(self.results),\n",
        "            'successful': len(successful),\n",
        "            'failed': len(self.results) - len(successful),\n",
        "            'predictions': {\n",
        "                'yes': predictions.count('yes'),\n",
        "                'no': predictions.count('no'),\n",
        "                'unclear': predictions.count('unclear')\n",
        "            },\n",
        "            'avg_latency_ms': sum(latencies) / len(latencies) if latencies else 0\n",
        "        }\n",
        "\n",
        "print(\"âœ… Evaluator loaded\")"
      ],
      "metadata": {
        "id": "eval-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â–¶ï¸ Run Evaluation"
      ],
      "metadata": {
        "id": "run-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    \"\"\"Main evaluation pipeline.\"\"\"\n",
        "    # Validate config\n",
        "    if not config.api_key:\n",
        "        raise ValueError(\"Please set your API key in the config!\")\n",
        "    \n",
        "    # Discover images\n",
        "    images = discover_images(config.image_folder)\n",
        "    if not images:\n",
        "        raise ValueError(f\"No images found in {config.image_folder}\")\n",
        "    \n",
        "    print(f\"ðŸ“ Found {len(images)} images\")\n",
        "    print(f\"ðŸ¤– Using model: {config.model_type.value}\")\n",
        "    \n",
        "    # Run evaluation\n",
        "    async with get_client(config.model_type, config.api_key) as client:\n",
        "        evaluator = Evaluator(client, config)\n",
        "        await evaluator.run(images)\n",
        "        \n",
        "        # Save and summarize\n",
        "        evaluator.save_results(config.output_folder)\n",
        "        summary = evaluator.summary()\n",
        "        \n",
        "        print(\"\\nðŸ“Š Summary:\")\n",
        "        print(json.dumps(summary, indent=2))\n",
        "    \n",
        "    return evaluator\n",
        "\n",
        "# Run the evaluation\n",
        "evaluator = await main()"
      ],
      "metadata": {
        "id": "run-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Visualization"
      ],
      "metadata": {
        "id": "viz-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_results(evaluator: Evaluator):\n",
        "    \"\"\"Create visualizations of evaluation results.\"\"\"\n",
        "    df = pd.DataFrame([asdict(r) for r in evaluator.results])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # Prediction distribution\n",
        "    ax1 = axes[0]\n",
        "    pred_counts = df['prediction'].value_counts()\n",
        "    colors = {'yes': '#2ecc71', 'no': '#e74c3c', 'unclear': '#f39c12', 'error': '#95a5a6'}\n",
        "    pred_counts.plot(kind='bar', ax=ax1, color=[colors.get(x, '#333') for x in pred_counts.index])\n",
        "    ax1.set_title('Prediction Distribution')\n",
        "    ax1.set_xlabel('Prediction')\n",
        "    ax1.set_ylabel('Count')\n",
        "    ax1.tick_params(axis='x', rotation=0)\n",
        "    \n",
        "    # Latency distribution\n",
        "    ax2 = axes[1]\n",
        "    successful = df[df['success'] == True]\n",
        "    if not successful.empty:\n",
        "        sns.histplot(successful['latency_ms'], bins=20, ax=ax2, color='#3498db')\n",
        "        ax2.axvline(successful['latency_ms'].mean(), color='red', linestyle='--', label=f'Mean: {successful[\"latency_ms\"].mean():.0f}ms')\n",
        "        ax2.legend()\n",
        "    ax2.set_title('Latency Distribution')\n",
        "    ax2.set_xlabel('Latency (ms)')\n",
        "    ax2.set_ylabel('Count')\n",
        "    \n",
        "    # Success rate\n",
        "    ax3 = axes[2]\n",
        "    success_counts = df['success'].value_counts()\n",
        "    ax3.pie(\n",
        "        success_counts.values,\n",
        "        labels=['Success' if x else 'Failed' for x in success_counts.index],\n",
        "        autopct='%1.1f%%',\n",
        "        colors=['#2ecc71', '#e74c3c']\n",
        "    )\n",
        "    ax3.set_title('Success Rate')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(Path(config.output_folder) / 'results_visualization.png', dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Visualize if we have results\n",
        "if 'evaluator' in dir() and evaluator.results:\n",
        "    df = visualize_results(evaluator)\n",
        "    display(df.head(10))"
      ],
      "metadata": {
        "id": "viz-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ðŸ¦™ LLaVA Local Setup\n",
        "\n",
        "For running LLaVA locally (on Colab GPU or edge devices like Raspberry Pi)."
      ],
      "metadata": {
        "id": "llava-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone and setup LLaVA (for Colab with GPU)\n",
        "!git clone -b dev https://github.com/camenduru/LLaVA /content/LLaVA\n",
        "%cd /content/LLaVA\n",
        "\n",
        "!pip install -q transformers==4.36.2 ninja\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "llava-setup-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLaVA Evaluation Script\n",
        "LLAVA_EVAL_SCRIPT = '''\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
        "from llava.conversation import conv_templates, SeparatorStyle\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n",
        "from PIL import Image\n",
        "\n",
        "class LLaVAEvaluator:\n",
        "    def __init__(self, model_path: str, load_4bit: bool = True):\n",
        "        disable_torch_init()\n",
        "        self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(\n",
        "            model_path, None, \"llava\", load_4bit=load_4bit\n",
        "        )\n",
        "        self.conv_mode = \"llava_v1\"\n",
        "    \n",
        "    def evaluate(self, image_path: str, prompt: str) -> str:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_tensor = self.image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
        "        \n",
        "        inp = DEFAULT_IMAGE_TOKEN + \"\\\\n\" + prompt\n",
        "        conv = conv_templates[self.conv_mode].copy()\n",
        "        conv.append_message(conv.roles[0], inp)\n",
        "        conv.append_message(conv.roles[1], None)\n",
        "        \n",
        "        input_ids = tokenizer_image_token(\n",
        "            conv.get_prompt(), self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
        "        ).unsqueeze(0).cuda()\n",
        "        \n",
        "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
        "        stopping_criteria = KeywordsStoppingCriteria([stop_str], self.tokenizer, input_ids)\n",
        "        \n",
        "        with torch.inference_mode():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                images=image_tensor,\n",
        "                do_sample=True,\n",
        "                temperature=0.2,\n",
        "                max_new_tokens=100,\n",
        "                stopping_criteria=[stopping_criteria]\n",
        "            )\n",
        "        \n",
        "        return self.tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
        "    \n",
        "    def run_batch(self, image_folder: str, output_csv: str, prompt: str):\n",
        "        images = list(Path(image_folder).glob(\"*.png\")) + list(Path(image_folder).glob(\"*.jpg\"))\n",
        "        \n",
        "        with open(output_csv, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"image_name\", \"prediction\"])\n",
        "            \n",
        "            for img_path in tqdm(images, desc=\"Processing\"):\n",
        "                try:\n",
        "                    result = self.evaluate(str(img_path), prompt)\n",
        "                    writer.writerow([img_path.name, result])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {img_path.name}: {e}\")\n",
        "                    writer.writerow([img_path.name, f\"ERROR: {e}\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluator = LLaVAEvaluator(\"4bit/llava-v1.5-13b-3GB\")\n",
        "    evaluator.run_batch(\n",
        "        \"/content/images\",\n",
        "        \"/content/llava_results.csv\",\n",
        "        \"Is this a real traffic sign? Answer yes or no.\"\n",
        "    )\n",
        "'''\n",
        "\n",
        "# Save the script\n",
        "with open('/content/LLaVA/evaluate_traffic_signs.py', 'w') as f:\n",
        "    f.write(LLAVA_EVAL_SCRIPT)\n",
        "\n",
        "print(\"âœ… LLaVA evaluation script saved\")"
      ],
      "metadata": {
        "id": "llava-script-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run LLaVA evaluation\n",
        "%cd /content/LLaVA\n",
        "!python evaluate_traffic_signs.py"
      ],
      "metadata": {
        "id": "llava-run-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ðŸ”§ Edge Device Deployment (llama.cpp)\n",
        "\n",
        "For Raspberry Pi and NVIDIA Jetson devices."
      ],
      "metadata": {
        "id": "edge-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build llama.cpp for edge deployment\n",
        "!git clone https://github.com/ggerganov/llama.cpp /content/llama.cpp\n",
        "%cd /content/llama.cpp\n",
        "!mkdir -p build && cd build && cmake .. && cmake --build . --config Release -j4"
      ],
      "metadata": {
        "id": "llamacpp-build-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge deployment shell script\n",
        "EDGE_SCRIPT = '''#!/bin/bash\n",
        "# LLaVA Edge Evaluation Script\n",
        "# For Raspberry Pi / Jetson / ZED Box\n",
        "\n",
        "set -e\n",
        "\n",
        "# Configuration\n",
        "LLAVA_CLI=\"./build/bin/llava-cli\"\n",
        "MODEL=\"ggml-model-q4_k.gguf\"\n",
        "MMPROJ=\"mmproj-model-f16.gguf\"\n",
        "IMAGE_DIR=\"${1:-./images}\"\n",
        "OUTPUT_CSV=\"${2:-./results.csv}\"\n",
        "PROMPT=\"Is this a real traffic sign? Answer yes or no.\"\n",
        "\n",
        "# Check dependencies\n",
        "if [ ! -f \"$LLAVA_CLI\" ]; then\n",
        "    echo \"Error: llava-cli not found. Build llama.cpp first.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "if [ ! -f \"$MODEL\" ]; then\n",
        "    echo \"Downloading model from HuggingFace...\"\n",
        "    wget -q https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/ggml-model-q4_k.gguf\n",
        "    wget -q https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/mmproj-model-f16.gguf\n",
        "fi\n",
        "\n",
        "# Initialize CSV\n",
        "echo \"image_name,prediction,raw_output\" > \"$OUTPUT_CSV\"\n",
        "\n",
        "# Process images\n",
        "total=$(find \"$IMAGE_DIR\" -type f \\( -name \"*.png\" -o -name \"*.jpg\" \\) | wc -l)\n",
        "count=0\n",
        "\n",
        "for img in \"$IMAGE_DIR\"/*.{png,jpg,jpeg}; do\n",
        "    [ -f \"$img\" ] || continue\n",
        "    \n",
        "    count=$((count + 1))\n",
        "    basename=$(basename \"$img\")\n",
        "    echo \"[$count/$total] Processing: $basename\"\n",
        "    \n",
        "    # Run inference\n",
        "    output=$($LLAVA_CLI -m \"$MODEL\" --mmproj \"$MMPROJ\" --image \"$img\" -p \"$PROMPT\" 2>/dev/null || echo \"ERROR\")\n",
        "    \n",
        "    # Extract yes/no\n",
        "    if echo \"$output\" | grep -iq \"^yes\"; then\n",
        "        prediction=\"yes\"\n",
        "    elif echo \"$output\" | grep -iq \"^no\"; then\n",
        "        prediction=\"no\"\n",
        "    else\n",
        "        prediction=\"unclear\"\n",
        "    fi\n",
        "    \n",
        "    # Escape output for CSV\n",
        "    escaped_output=$(echo \"$output\" | tr \"\\n\" \" \" | sed 's/\"/\"\"/g')\n",
        "    \n",
        "    echo \"$basename,$prediction,\\\"$escaped_output\\\"\" >> \"$OUTPUT_CSV\"\n",
        "done\n",
        "\n",
        "echo \"\\nâœ… Evaluation complete. Results saved to $OUTPUT_CSV\"\n",
        "'''\n",
        "\n",
        "with open('/content/llama.cpp/run_evaluation.sh', 'w') as f:\n",
        "    f.write(EDGE_SCRIPT)\n",
        "\n",
        "!chmod +x /content/llama.cpp/run_evaluation.sh\n",
        "print(\"âœ… Edge deployment script saved\")\n",
        "print(\"\\nUsage: ./run_evaluation.sh /path/to/images /path/to/output.csv\")"
      ],
      "metadata": {
        "id": "edge-script-cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
